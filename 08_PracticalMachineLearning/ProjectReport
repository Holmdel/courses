## Project Report

## I began by using the full training data set and variables in my classification exercises. 
## My computer's capacity was compromised and I was unable to obtain any results. I chose a subset of
## the variables and reduced the number of observations and ran models using the random forest 
## classification with good results.

## The datasets were loaded into Excel where I reduced the number of columns (31 predictors + one outcome)
## and saved the edited files as .csv files
library(caret)
library(randomForest)
setwd("C:/Users/Frank/Documents/R/machine learning")
file1 <- "pml-training-red1.csv"
file2 <- "pml-testing-red1.csv"

train <- read.csv(file1,header = TRUE)
test <- read.csv(file2,header = TRUE)

## The dataset was reduced to 12,000 rows and then split to enable cross validation.
trainSample <- train[sample(nrow(train),12000),]
inTrain <- createDataPartition(y=trainSample$classe,p=0.7, list=FALSE)
training <- trainSample[inTrain,]
testing <- trainSample[-inTrain,]
dim(training); dim(testing)
## [1] 8403   32
## [1] 3597   32

## The first column was stripped after it proved to bias the results due to its high correlation with classe.
training <- training[,-1]
testing <- testing[,-1]
test <- test[,-1]

## randomForest was chosen due to its efficiency and results. I had initially used the train function but
## it seemed to take forever and allowed me little time to test different sizes and predictors.
fit <- randomForest(classe ~ ., prox=T,data = training)
pred <- predict(fit,training)
pred1 <- predict(fit,testing)

## Inspecting the fit result showed a reasonable error rate.
## > fit

Call:
 randomForest(formula = classe ~ ., data = training, prox = T) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 5

        OOB estimate of  error rate: 3.18%
Confusion matrix:
     A    B    C    D    E class.error
A 2370    5    1    2    0 0.003364172
B   57 1533   22    3    6 0.054287477
C    7   29 1429    2    6 0.029871012
D    3    0   56 1305   17 0.055032585
E    2   14   10   25 1499 0.032903226

#I did cross validation using the confusion matrix on the testing dataset.
conf <- table(pred1,testing$classe)
confusionMatrix(conf)

Confusion Matrix and Statistics

     
pred1    A    B    C    D    E
    A 1010   12    2    1    2
    B    6  670    8    0    7
    C    2    9  616   25    4
    D    0    0    2  561   12
    E    0    3    2    4  639

Overall Statistics
                                         
               Accuracy : 0.9719         
                 95% CI : (0.966, 0.9771)
    No Information Rate : 0.283          
    P-Value [Acc > NIR] : < 2.2e-16      
                                         
                  Kappa : 0.9645         
 Mcnemar's Test P-Value : NA             

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9921   0.9654   0.9778   0.9492   0.9623
Specificity            0.9934   0.9928   0.9865   0.9953   0.9969
Pos Pred Value         0.9834   0.9696   0.9390   0.9757   0.9861
Neg Pred Value         0.9969   0.9917   0.9952   0.9901   0.9915
Prevalence             0.2830   0.1929   0.1751   0.1643   0.1846
Detection Rate         0.2808   0.1863   0.1713   0.1560   0.1776
Detection Prevalence   0.2855   0.1921   0.1824   0.1599   0.1802
Balanced Accuracy      0.9928   0.9791   0.9821   0.9723   0.9796

## I accepted the 97.2% accuracy and applied the fit model to the test dataset.
prediction <- as.character(predict(fit,test))
prediction
 [1] "B" "A" "B" "A" "A" "E" "D" "B" "A" "A" "B" "C" "B" "A" "E" "E" "A" "B" "B" "B"
